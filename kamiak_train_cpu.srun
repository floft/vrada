#!/bin/bash
#SBATCH --job-name=VRADA
#SBATCH --output=slurm_logs/vrada_%j.out
#SBATCH --error=slurm_logs/vrada_%j.err
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=12
#SBATCH --nodes=1-1
#SBATCH --gres=gpu:tesla:0
#SBATCH --partition=taylor,cahnrs_gpu,free_gpu,kamiak,vcea,cahnrs,cahnrs_bigmem
#SBATCH --time=4-00:00:00
#SBATCH --mem=50G

. kamiak_config.sh

#
# ---
#

first_arg=$1
if [[ -z $first_arg ]]; then
    echo "Specify what method and dataset to use, e.g. --vrnn-da --trivial"
    exit 1
else
    echo "Args: $@"
fi

# Allow overriding the dataset zip file with --dataset=... or additionally
# the --logdir=..., --modeldir=..., and --imgdir=... Any other args (e.g.
# --vrada-da) are passed directly to the Python program.
program_args=()
for i; do
    name="$(cut -d'=' -f1 <<< "$i")"
    value="$(cut -d'=' -f2 <<< "$i")"

    if [[ "$name" == "--dataset" ]]; then
        compressedDataset="$value"
        echo "Overriding dataset to be: $compressedDataset"
    elif [[ "$name" == "--logdir" ]]; then
        logFolder="$value"
        echo "Overriding logdir to be: $logFolder"
    elif [[ "$name" == "--modeldir" ]]; then
        modelFolder="$value"
        echo "Overriding modeldir to be: $modelFolder"
    else
        program_args+=("$i")
    fi

    shift
done

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
module load python3/3.5.0
data="$remotedir"

function clean_up
{
    rmworkspace -a -f --name="$SCRATCHDIR"
    exit
}

# Create a scratch workspace
SCRATCHDIR="$(mkworkspace -q -t 7-00:00 -b /local)" # 7 days
trap 'clean_up' EXIT

echo "Scratch space: $SCRATCHDIR"
echo "SLURM_CPUS_PER_TASK: $SLURM_CPUS_PER_TASK"
echo "SLURM_JOB_GPUS: $SLURM_JOB_GPUS"

# This is somewhat sensitive data
chmod 0700 "$SCRATCHDIR"

# Get data
echo "Getting data: started"
cd "$SCRATCHDIR"
echo " - program"
cp -a "$data"/*.py .
echo " - dataset"
cp -a "$data/$compressedDataset" .
unzip "$compressedDataset"
echo "Getting data: done"

# Install dependencies
echo "Making sure TensorFlow installed: starting"
pip install --user virtualenvwrapper
export VIRTUALENVWRAPPER_PYTHON="$(which python3)"
export WORKON_HOME=~/Envs
source ~/.local/bin/virtualenvwrapper.sh
if [[ ! -e ~/Envs/tensorflow_cpu ]]; then
    mkvirtualenv -p python3 tensorflow_cpu
else
    workon tensorflow_cpu
fi
# Seems to hang on numpy without --no-cache-dir?
pip --no-cache-dir install tensorflow pillow lxml jupyter matplotlib pandas sklearn scipy
echo "Making sure TensorFlow installed: done"

# Train
echo "Training network: started"
mkdir -p "$data/$logFolder/" # log dir, rsync this to view with TensorBoard
python3 "$program" --logdir "$data/$logFolder" --modeldir "$data/$modelFolder" \
    --debug "${program_args[@]}"
echo "Training network: done"

# Cleanup
echo "Deleting workspace: started"
clean_up
echo "Deleting workspace: done"
